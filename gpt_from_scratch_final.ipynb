{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dzaYKFVqg22",
        "outputId": "e72b719d-54bf-4d75-d756-f4fe4f45b9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  GPT-from-scratch-implementation-and-tutorial-main.zip\n",
            "ac69a0192d80f264ef626b5223a78d40d9d74493\n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/.DS_Store  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/GPT from Scratch Implementation Report.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/README.md  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/Shakespeare_clean_full.txt  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/Shakespeare_clean_test.txt  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/Shakespeare_clean_train.txt  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/Task1_22.7.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/Task2_23.7.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/Task3_29.07.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/Task4_31.07.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/__init__.py  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/__pycache__/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/__pycache__/generate_text.cpython-39.pyc  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/__pycache__/task2.cpython-39.pyc  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/__pycache__/task3.cpython-39.pyc  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/__pycache__/task4.cpython-39.pyc  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/__pycache__/utils.cpython-39.pyc  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/clean_nltk_shakespear_data.py  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/.gitignore  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/GPT_Implementation_Article.md  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/README.md  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/components.json  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/eslint.config.mjs  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/next.config.ts  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/package-lock.json  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/package.json  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/postcss.config.mjs  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/file.svg  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/globe.svg  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/next.svg  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/report.md  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task3_merges1000_lr0.0001.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task3_merges1000_lr0.0005.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task3_merges1000_lr5e-05.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task3_merges2000_lr0.0001.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task3_merges2000_lr0.0005.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task3_merges2000_lr5e-05.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task4_merges1000_lr0.0003.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/task4_merges2000_lr0.0003.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/vercel.svg  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/public/window.svg  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/\n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/app/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/app/favicon.ico  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/app/globals.css  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/app/layout.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/app/page.tsx  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/FunctionNode.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/NodeDetailsDrawer.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/TaskFlow.tsx  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/badge.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/button.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/card.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/dialog.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/scroll-area.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/separator.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/sheet.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/tabs.tsx  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/components/ui/tooltip.tsx  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/data/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/data/report.ts  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/data/tasks.ts  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/lib/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/src/lib/utils.ts  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/tailwind.config.ts  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt-report/tsconfig.json  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt_from_scratch_1.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt_from_scratch_2.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt_from_scratch_3.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt_from_scratch_4.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt_from_scratch_5.pdf  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/gpt_from_scratch_final.ipynb  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/requirements.txt  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task1.py  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task2.py  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3.py  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots.zip  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots/task3_merges1000_lr0.0001.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots/task3_merges1000_lr0.0005.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots/task3_merges1000_lr5e-05.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots/task3_merges2000_lr0.0001.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots/task3_merges2000_lr0.0005.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task3_plots/task3_merges2000_lr5e-05.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task4.py  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task4_plots.zip  \n",
            "   creating: GPT-from-scratch-implementation-and-tutorial-main/task4_plots/\n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task4_plots/task4_merges1000_lr0.0003.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/task4_plots/task4_merges2000_lr0.0003.png  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/test_bpe_simple.py  \n",
            "  inflating: GPT-from-scratch-implementation-and-tutorial-main/utils.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip GPT-from-scratch-implementation-and-tutorial-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd GPT-from-scratch-implementation-and-tutorial-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ma-O7kqnDn",
        "outputId": "1d317843-effc-4e79-c7d7-bdb348e367d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-from-scratch-implementation-and-tutorial-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIV8MpQqqnGO",
        "outputId": "92135612-b264-4333-ed41-ab3b8ed255c2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-from-scratch-implementation-and-tutorial-main\n",
            " bpe_cache_1000_lower_nopunct.pkl\t       task2_fixed_2000_3.pkl\n",
            " bpe_cache_2000_lower_nopunct.pkl\t       task2_fixed_2000_4.pkl\n",
            " bpe_cache_2500_lower_nopunct.pkl\t       task2_fixed_2500_1.pkl\n",
            " bpe_cache_500_lower_nopunct.pkl\t       task2_fixed_2500_2.pkl\n",
            " clean_nltk_shakespear_data.py\t\t       task2_fixed_2500_3.pkl\n",
            " gpt_from_scratch_1.pdf\t\t\t       task2_fixed_2500_4.pkl\n",
            " gpt_from_scratch_2.pdf\t\t\t       task2_fixed_500_1.pkl\n",
            " gpt_from_scratch_3.pdf\t\t\t       task2_fixed_500_2.pkl\n",
            " gpt_from_scratch_4.pdf\t\t\t       task2_fixed_500_3.pkl\n",
            " gpt_from_scratch_5.pdf\t\t\t       task2_fixed_500_4.pkl\n",
            " gpt_from_scratch_final.ipynb\t\t       task2_fixed_results.pkl\n",
            "'GPT from Scratch Implementation Report.pdf'   task2.py\n",
            " gpt-report\t\t\t\t       task3_1000_lr0.0005_final.pt\n",
            " __init__.py\t\t\t\t       task3_1000_lr0.001_final.pt\n",
            " __pycache__\t\t\t\t       task3_2000_lr0.0005_final.pt\n",
            " README.md\t\t\t\t       task3_2000_lr0.001_final.pt\n",
            " requirements.txt\t\t\t       Task3_29.07.pdf\n",
            " Shakespeare_clean_full.txt\t\t       task3_500_lr0.0005_final.pt\n",
            " Shakespeare_clean_test.txt\t\t       task3_500_lr0.001_final.pt\n",
            " Shakespeare_clean_train.txt\t\t       task3_plots\n",
            " Task1_22.7.pdf\t\t\t\t       task3_plots.zip\n",
            " task1.py\t\t\t\t       task3.py\n",
            " task1_results.pkl\t\t\t       task3_results.pkl\n",
            " Task2_23.7.pdf\t\t\t\t       Task4_31.07.pdf\n",
            " task2_fixed_1000_1.pkl\t\t\t       task4_plots\n",
            " task2_fixed_1000_2.pkl\t\t\t       task4_plots.zip\n",
            " task2_fixed_1000_3.pkl\t\t\t       task4.py\n",
            " task2_fixed_1000_4.pkl\t\t\t       test_bpe_simple.py\n",
            " task2_fixed_2000_1.pkl\t\t\t       utils.py\n",
            " task2_fixed_2000_2.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download all BPE models at once (trained on all of Shakespeare training data)\n",
        "\n",
        "# # lower_nopunct\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1h2UeTk9FLzYlPz5KcR-1TRLAR8EFttM1' -O bpe_cache_1000_lower_nopunct.pkl\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1N34p7aQdCwnVwEsgxE-yjBmGwrhnIFpc' -O bpe_cache_2000_lower_nopunct.pkl\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1cEJG6Xg8kFTDWJXX_7yrT0o9TfY-uMSl' -O bpe_cache_3000_lower_nopunct.pkl\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=<LOWER_NOPUNCT_5000_ID>' -O bpe_cache_5000_lower_nopunct.pkl\n",
        "\n",
        "# # aggressive\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1ZNKmi_lztzXVnoYaNnDQyNb6Y-mT1DxJ' -O bpe_cache_1000_aggressive.pkl\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1t7-_RjL2v-lIfBdThiuXsqW14KpsGxRH' -O bpe_cache_2000_aggressive.pkl\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1eZCLEpe__SEJyFUKPVwFfYXl-U3xfPCa' -O bpe_cache_3000_aggressive.pkl\n",
        "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=<AGGRESSIVE_5000_ID>' -O bpe_cache_5000_aggressive.pkl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO_15evWqnLe",
        "outputId": "21e87ee5-532a-48be-a8fa-2e74a3ec01d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-02 09:05:14--  https://drive.google.com/uc?export=download&id=1h2UeTk9FLzYlPz5KcR-1TRLAR8EFttM1\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1h2UeTk9FLzYlPz5KcR-1TRLAR8EFttM1&export=download [following]\n",
            "--2025-09-02 09:05:14--  https://drive.usercontent.google.com/download?id=1h2UeTk9FLzYlPz5KcR-1TRLAR8EFttM1&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100572 (98K) [application/octet-stream]\n",
            "Saving to: ‘bpe_cache_1000_lower_nopunct.pkl’\n",
            "\n",
            "bpe_cache_1000_lowe 100%[===================>]  98.21K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-09-02 09:05:16 (135 MB/s) - ‘bpe_cache_1000_lower_nopunct.pkl’ saved [100572/100572]\n",
            "\n",
            "--2025-09-02 09:05:16--  https://drive.google.com/uc?export=download&id=1N34p7aQdCwnVwEsgxE-yjBmGwrhnIFpc\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1N34p7aQdCwnVwEsgxE-yjBmGwrhnIFpc&export=download [following]\n",
            "--2025-09-02 09:05:16--  https://drive.usercontent.google.com/download?id=1N34p7aQdCwnVwEsgxE-yjBmGwrhnIFpc&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100572 (98K) [application/octet-stream]\n",
            "Saving to: ‘bpe_cache_2000_lower_nopunct.pkl’\n",
            "\n",
            "bpe_cache_2000_lowe 100%[===================>]  98.21K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-09-02 09:05:17 (122 MB/s) - ‘bpe_cache_2000_lower_nopunct.pkl’ saved [100572/100572]\n",
            "\n",
            "--2025-09-02 09:05:17--  https://drive.google.com/uc?export=download&id=1cEJG6Xg8kFTDWJXX_7yrT0o9TfY-uMSl\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1cEJG6Xg8kFTDWJXX_7yrT0o9TfY-uMSl&export=download [following]\n",
            "--2025-09-02 09:05:17--  https://drive.usercontent.google.com/download?id=1cEJG6Xg8kFTDWJXX_7yrT0o9TfY-uMSl&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65992 (64K) [application/octet-stream]\n",
            "Saving to: ‘bpe_cache_3000_lower_nopunct.pkl’\n",
            "\n",
            "bpe_cache_3000_lowe 100%[===================>]  64.45K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-09-02 09:05:19 (56.4 MB/s) - ‘bpe_cache_3000_lower_nopunct.pkl’ saved [65992/65992]\n",
            "\n",
            "--2025-09-02 09:05:19--  https://drive.google.com/uc?export=download&id=%3CLOWER_NOPUNCT_5000_ID%3E\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-09-02 09:05:19 ERROR 404: Not Found.\n",
            "\n",
            "--2025-09-02 09:05:19--  https://drive.google.com/uc?export=download&id=1ZNKmi_lztzXVnoYaNnDQyNb6Y-mT1DxJ\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1ZNKmi_lztzXVnoYaNnDQyNb6Y-mT1DxJ&export=download [following]\n",
            "--2025-09-02 09:05:19--  https://drive.usercontent.google.com/download?id=1ZNKmi_lztzXVnoYaNnDQyNb6Y-mT1DxJ&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65992 (64K) [application/octet-stream]\n",
            "Saving to: ‘bpe_cache_1000_aggressive.pkl’\n",
            "\n",
            "bpe_cache_1000_aggr 100%[===================>]  64.45K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-09-02 09:05:20 (85.2 MB/s) - ‘bpe_cache_1000_aggressive.pkl’ saved [65992/65992]\n",
            "\n",
            "--2025-09-02 09:05:20--  https://drive.google.com/uc?export=download&id=1t7-_RjL2v-lIfBdThiuXsqW14KpsGxRH\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1t7-_RjL2v-lIfBdThiuXsqW14KpsGxRH&export=download [following]\n",
            "--2025-09-02 09:05:20--  https://drive.usercontent.google.com/download?id=1t7-_RjL2v-lIfBdThiuXsqW14KpsGxRH&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31366 (31K) [application/octet-stream]\n",
            "Saving to: ‘bpe_cache_2000_aggressive.pkl’\n",
            "\n",
            "bpe_cache_2000_aggr 100%[===================>]  30.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-02 09:05:22 (92.3 MB/s) - ‘bpe_cache_2000_aggressive.pkl’ saved [31366/31366]\n",
            "\n",
            "--2025-09-02 09:05:22--  https://drive.google.com/uc?export=download&id=1eZCLEpe__SEJyFUKPVwFfYXl-U3xfPCa\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1eZCLEpe__SEJyFUKPVwFfYXl-U3xfPCa&export=download [following]\n",
            "--2025-09-02 09:05:22--  https://drive.usercontent.google.com/download?id=1eZCLEpe__SEJyFUKPVwFfYXl-U3xfPCa&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31366 (31K) [application/octet-stream]\n",
            "Saving to: ‘bpe_cache_3000_aggressive.pkl’\n",
            "\n",
            "bpe_cache_3000_aggr 100%[===================>]  30.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-02 09:05:23 (122 MB/s) - ‘bpe_cache_3000_aggressive.pkl’ saved [31366/31366]\n",
            "\n",
            "--2025-09-02 09:05:23--  https://drive.google.com/uc?export=download&id=%3CAGGRESSIVE_5000_ID%3E\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.202.101, 173.194.202.113, 173.194.202.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.202.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-09-02 09:05:23 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf *pkl *.pt"
      ],
      "metadata": {
        "id": "RVexKD_MvRPx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python task1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTClFTIqqnOR",
        "outputId": "e107f865-84f5-49ef-ed9f-ae0de47f5fdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: BPE Tokenization\n",
            "==================================================\n",
            "Using 100.0% of each split | chars: train=864424, valid=51965, test=52008\n",
            "\n",
            "Testing BPE with 500 merges and lower_nopunct normalization...\n",
            "Loaded cached BPE: 500 merges, lower_nopunct normalization\n",
            "================================================================\n",
            "CONFIG  normalization_technique=lower_nopunct | merges(merge_count)=500\n",
            "================================================================\n",
            "[SUMMARY] vocab=454\n",
            "  avg tokens/word: train=2.0027 (N=162942) |   avg tokens/word: valid=2.0236 (N=9759) |   avg tokens/word: test=1.9582 (N=9804) | \n",
            "  reconstruct_ok: train=True |   reconstruct_ok: valid=True |   reconstruct_ok: test=True | \n",
            "\n",
            "Testing BPE with 1000 merges and lower_nopunct normalization...\n",
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "================================================================\n",
            "CONFIG  normalization_technique=lower_nopunct | merges(merge_count)=1000\n",
            "================================================================\n",
            "[SUMMARY] vocab=529\n",
            "  avg tokens/word: train=1.8998 (N=162942) |   avg tokens/word: valid=1.9272 (N=9759) |   avg tokens/word: test=1.8634 (N=9804) | \n",
            "  reconstruct_ok: train=True |   reconstruct_ok: valid=True |   reconstruct_ok: test=True | \n",
            "\n",
            "Testing BPE with 2000 merges and lower_nopunct normalization...\n",
            "Loaded cached BPE: 2000 merges, lower_nopunct normalization\n",
            "================================================================\n",
            "CONFIG  normalization_technique=lower_nopunct | merges(merge_count)=2000\n",
            "================================================================\n",
            "[SUMMARY] vocab=373\n",
            "  avg tokens/word: train=1.8937 (N=162942) |   avg tokens/word: valid=1.9272 (N=9759) |   avg tokens/word: test=1.8634 (N=9804) | \n",
            "  reconstruct_ok: train=True |   reconstruct_ok: valid=True |   reconstruct_ok: test=True | \n",
            "\n",
            "Testing BPE with 2500 merges and lower_nopunct normalization...\n",
            "Loaded cached BPE: 2500 merges, lower_nopunct normalization\n",
            "================================================================\n",
            "CONFIG  normalization_technique=lower_nopunct | merges(merge_count)=2500\n",
            "================================================================\n",
            "[SUMMARY] vocab=141\n",
            "  avg tokens/word: train=1.8906 (N=162942) |   avg tokens/word: valid=1.9272 (N=9759) |   avg tokens/word: test=1.8634 (N=9804) | \n",
            "  reconstruct_ok: train=True |   reconstruct_ok: valid=True |   reconstruct_ok: test=True | \n",
            "Results saved to task1_results.pkl\n",
            "\n",
            "Best configuration:\n",
            "  Normalization: lower_nopunct\n",
            "  Merge count: 1000\n",
            "  Validation avg tokens/word: 1.9272\n",
            "\n",
            "Task 1 completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDhpXaArqnRA",
        "outputId": "13e864fc-d68a-4c86-dcd3-ededf54927d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Move all BPE cache files to Google Drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "%rm -rf \"/content/drive/MyDrive/bpe_models\"\n",
        "\n",
        "# Create a directory for BPE models in Drive if it doesn't exist\n",
        "drive_bpe_dir = \"/content/drive/MyDrive/bpe_models\"\n",
        "os.makedirs(drive_bpe_dir, exist_ok=True)\n",
        "\n",
        "# Find and copy all BPE cache files\n",
        "bpe_files = [f for f in os.listdir('.') if f.startswith('bpe_cache_') and f.endswith('.pkl')]\n",
        "\n",
        "for file in bpe_files:\n",
        "    source_path = file\n",
        "    dest_path = os.path.join(drive_bpe_dir, file)\n",
        "    shutil.copy2(source_path, dest_path)\n",
        "    print(f\"Copied {file} to Drive\")\n",
        "\n",
        "print(f\"\\nTotal BPE files moved: {len(bpe_files)}\")\n",
        "print(f\"Files moved to: {drive_bpe_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spPIlRwbqnTW",
        "outputId": "e5fe00dd-fcfe-41bb-fb89-9177d5675fcd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied bpe_cache_500_lower_nopunct.pkl to Drive\n",
            "Copied bpe_cache_1000_lower_nopunct.pkl to Drive\n",
            "Copied bpe_cache_2500_lower_nopunct.pkl to Drive\n",
            "Copied bpe_cache_2000_lower_nopunct.pkl to Drive\n",
            "\n",
            "Total BPE files moved: 4\n",
            "Files moved to: /content/drive/MyDrive/bpe_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import load_cached_bpe, GEN_CONTEXT, normalize_generation_context\n",
        "\n",
        "def test_bpe(count):\n",
        "    bpe = load_cached_bpe(count, \"lower_nopunct\")\n",
        "    if bpe:\n",
        "        text = GEN_CONTEXT\n",
        "        encoded = bpe.encode(text)\n",
        "        decoded = bpe.decode(encoded)\n",
        "        print(f\"Original: {text}\")\n",
        "        print(f\"Encoded: {encoded}\")\n",
        "        print(f\"Decoded: {decoded}\")\n",
        "        print(f\"Vocab size: {len(bpe.vocab)}\")\n",
        "    else:\n",
        "        print(\"BPE not found\")\n",
        "\n",
        "for count in [500, 1000]:\n",
        "    print('--------', count)\n",
        "    test_bpe(count)\n",
        "    print('--------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn7zQGhhwZNs",
        "outputId": "d7c1160c-71b5-4113-fc32-6cbd4ca770c2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------- 500\n",
            "Loaded cached BPE: 500 merges, lower_nopunct normalization\n",
            "Original: to be or not to be \n",
            "Encoded: ['to ', 'be ', 'or ', 'not ', 'to ', 'be ']\n",
            "Decoded: to be or not to be \n",
            "Vocab size: 454\n",
            "--------\n",
            "-------- 1000\n",
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "Original: to be or not to be \n",
            "Encoded: ['to ', 'be ', 'or ', 'not ', 'to ', 'be ']\n",
            "Decoded: to be or not to be \n",
            "Vocab size: 529\n",
            "--------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import load_cached_bpe\n",
        "\n",
        "bpe = load_cached_bpe(1000, \"lower_nopunct\")\n",
        "\n",
        "test_context = \"fair is foul and  \"\n",
        "encoded = bpe.encode(test_context)\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", bpe.decode(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfmLgXlNIK9d",
        "outputId": "c4fba343-639e-4d59-9702-f9487a333643"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "Encoded: ['fa', 'ir ', 'is ', 'f', 'ou', 'l ', 'and ']\n",
            "Decoded: fair is foul and \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python task2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rb5UjkerMuj",
        "outputId": "b0f20379-f306-4b7c-c436-6c640973e074"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2: N-gram Language Modeling (FIXED)\n",
            "============================================================\n",
            "Using 100.0% of each split | chars: train=864424, valid=51965, test=52008\n",
            "\n",
            "Using BPE merges=500\n",
            "Loaded cached BPE: 500 merges, lower_nopunct normalization\n",
            "  BPE vocab size: 454\n",
            "  Train tokens: 328430\n",
            "  Unique tokens in train: 510\n",
            "  Training 1-gram...\n",
            "    Actual vocab size from data: 511\n",
            "    Total n-grams: 328430\n",
            "    Unique n-grams: 510\n",
            "    Actual vocab: 511 | val_ppl=234.72 | test_ppl=236.19\n",
            "    [Generated]: to be or not to be iand is y gomrto e e monhe d and o krs\n",
            "  Training 2-gram...\n",
            "    Actual vocab size from data: 511\n",
            "    Total n-grams: 328430\n",
            "    Unique n-grams: 36263\n",
            "    Actual vocab: 511 | val_ppl=64.89 | test_ppl=69.49\n",
            "    [Generated]: to be or not to be a bout a soothsayer octavius dy mack and he is no eart dowas the st\n",
            "  Training 3-gram...\n",
            "    Actual vocab size from data: 511\n",
            "    Total n-grams: 328430\n",
            "    Unique n-grams: 162413\n",
            "    Actual vocab: 511 | val_ppl=139.89 | test_ppl=143.29\n",
            "    [Generated]: to be or not to be exantonprale spight worow vido ghhuuror ion e cebeloke dy \n",
            "  Training 4-gram...\n",
            "    Actual vocab size from data: 511\n",
            "    Total n-grams: 328430\n",
            "    Unique n-grams: 249783\n",
            "    Actual vocab: 511 | val_ppl=296.69 | test_ppl=279.86\n",
            "    [Generated]: to be or not to be ool beloved notfinhath parto forfortunmark antony cabrearprae of our aleeg1thou do\n",
            "\n",
            "Using BPE merges=1000\n",
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "  BPE vocab size: 529\n",
            "  Train tokens: 311188\n",
            "  Unique tokens in train: 635\n",
            "  Training 1-gram...\n",
            "    Actual vocab size from data: 636\n",
            "    Total n-grams: 311188\n",
            "    Unique n-grams: 635\n",
            "    Actual vocab: 636 | val_ppl=270.37 | test_ppl=274.92\n",
            "    [Generated]: to be or not to be and ise pmy and and fk but ssthe bpand tven t\n",
            "  Training 2-gram...\n",
            "    Actual vocab size from data: 636\n",
            "    Total n-grams: 311188\n",
            "    Unique n-grams: 43519\n",
            "    Actual vocab: 636 | val_ppl=80.15 | test_ppl=86.52\n",
            "    [Generated]: to be or not to be your wishes o a piday and men the men less messlet us as mp\n",
            "  Training 3-gram...\n",
            "    Actual vocab size from data: 636\n",
            "    Total n-grams: 311188\n",
            "    Unique n-grams: 170336\n",
            "    Actual vocab: 636 | val_ppl=195.89 | test_ppl=200.49\n",
            "    [Generated]: to be or not to be phisee ounurconlieform ere finsed d alexas ing m take asforesee our fortunes of pai \n",
            "  Training 4-gram...\n",
            "    Actual vocab size from data: 636\n",
            "    Total n-grams: 311188\n",
            "    Unique n-grams: 246363\n",
            "    Actual vocab: 636 | val_ppl=395.61 | test_ppl=372.26\n",
            "    [Generated]: to be or not to be attendant whman antonom r let me lord their on such ys lius lord eunulai ll loon eeds \n",
            "\n",
            "Using BPE merges=2000\n",
            "Loaded cached BPE: 2000 merges, lower_nopunct normalization\n",
            "  BPE vocab size: 373\n",
            "  Train tokens: 310188\n",
            "  Unique tokens in train: 628\n",
            "  Training 1-gram...\n",
            "    Actual vocab size from data: 629\n",
            "    Total n-grams: 310188\n",
            "    Unique n-grams: 628\n",
            "    Actual vocab: 629 | val_ppl=270.31 | test_ppl=274.93\n",
            "    [Generated]: to be or not to be mshit is instcsatisheithe erand be heathe their \n",
            "  Training 2-gram...\n",
            "    Actual vocab size from data: 629\n",
            "    Total n-grams: 310188\n",
            "    Unique n-grams: 43261\n",
            "    Actual vocab: 629 | val_ppl=79.87 | test_ppl=86.19\n",
            "    [Generated]: to be or not to be fair horatilust thou hamonether who did in her and \n",
            "  Training 3-gram...\n",
            "    Actual vocab size from data: 629\n",
            "    Total n-grams: 310188\n",
            "    Unique n-grams: 169654\n",
            "    Actual vocab: 629 | val_ppl=194.09 | test_ppl=198.57\n",
            "    [Generated]: to be or not to be deounwill ts to caesar ay attendant fulpomave ter ess nesoothsayer your wiman rates ly i ll ate \n",
            "  Training 4-gram...\n",
            "    Actual vocab size from data: 629\n",
            "    Total n-grams: 310188\n",
            "    Unique n-grams: 245472\n",
            "    Actual vocab: 629 | val_ppl=391.48 | test_ppl=368.28\n",
            "    [Generated]: to be or not to be ga pargive me amen deaventimuere some caesar your wisay e so spywhere him cleopatra s \n",
            "\n",
            "Using BPE merges=2500\n",
            "Loaded cached BPE: 2500 merges, lower_nopunct normalization\n",
            "  BPE vocab size: 141\n",
            "  Train tokens: 309688\n",
            "  Unique tokens in train: 617\n",
            "  Training 1-gram...\n",
            "    Actual vocab size from data: 618\n",
            "    Total n-grams: 309688\n",
            "    Unique n-grams: 617\n",
            "    Actual vocab: 618 | val_ppl=270.26 | test_ppl=274.90\n",
            "    [Generated]: to be or not to be a i de you s mit on ere i dwell the bibce d thy a\n",
            "  Training 2-gram...\n",
            "    Actual vocab size from data: 618\n",
            "    Total n-grams: 309688\n",
            "    Unique n-grams: 43082\n",
            "    Actual vocab: 618 | val_ppl=79.37 | test_ppl=85.66\n",
            "    [Generated]: to be or not to be ge he hath were are no by and bleainst the deed in re\n",
            "  Training 3-gram...\n",
            "    Actual vocab size from data: 618\n",
            "    Total n-grams: 309688\n",
            "    Unique n-grams: 169279\n",
            "    Actual vocab: 618 | val_ppl=191.05 | test_ppl=195.44\n",
            "    [Generated]: to be or not to be athis itthing k gaomekingrinotaprivcharmian iras ghts how should thwe ll not\n",
            "  Training 4-gram...\n",
            "    Actual vocab size from data: 618\n",
            "    Total n-grams: 309688\n",
            "    Unique n-grams: 245010\n",
            "    Actual vocab: 618 | val_ppl=384.82 | test_ppl=361.97\n",
            "    [Generated]: to be or not to be ctheir y de it is a charmian fe charmian a cuckold retherspeaantony and it is a pommadis caesar it is frll\n",
            "Results saved to task2_fixed_results.pkl\n",
            "\n",
            "============================================================\n",
            "SUMMARY:\n",
            "\n",
            "BPE merges=500 (BPE vocab=454)\n",
            "  n=1: actual_vocab=511 | val=234.72 | test=236.19\n",
            "  n=2: actual_vocab=511 | val=64.89 | test=69.49\n",
            "  n=3: actual_vocab=511 | val=139.89 | test=143.29\n",
            "  n=4: actual_vocab=511 | val=296.69 | test=279.86\n",
            "\n",
            "BPE merges=1000 (BPE vocab=529)\n",
            "  n=1: actual_vocab=636 | val=270.37 | test=274.92\n",
            "  n=2: actual_vocab=636 | val=80.15 | test=86.52\n",
            "  n=3: actual_vocab=636 | val=195.89 | test=200.49\n",
            "  n=4: actual_vocab=636 | val=395.61 | test=372.26\n",
            "\n",
            "BPE merges=2000 (BPE vocab=373)\n",
            "  n=1: actual_vocab=629 | val=270.31 | test=274.93\n",
            "  n=2: actual_vocab=629 | val=79.87 | test=86.19\n",
            "  n=3: actual_vocab=629 | val=194.09 | test=198.57\n",
            "  n=4: actual_vocab=629 | val=391.48 | test=368.28\n",
            "\n",
            "BPE merges=2500 (BPE vocab=141)\n",
            "  n=1: actual_vocab=618 | val=270.26 | test=274.90\n",
            "  n=2: actual_vocab=618 | val=79.37 | test=85.66\n",
            "  n=3: actual_vocab=618 | val=191.05 | test=195.44\n",
            "  n=4: actual_vocab=618 | val=384.82 | test=361.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python task3.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8G8ffvCsJme",
        "outputId": "697c1644-a7e0-456b-9827-6f770e7182c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 3: Neural Bigram Language Modeling (FIXED)\n",
            "==================================================\n",
            "Using device: cuda\n",
            "Using 100.0% of each split | chars: train=864424, valid=51965, test=52008\n",
            "\n",
            "BPE merges=500\n",
            "Loaded cached BPE: 500 merges, lower_nopunct normalization\n",
            "  Vocab size=510, train tokens=328429\n",
            "  Training with LR=0.001\n",
            "    Config: emb_dim=64, batch=32\n",
            "    Prepared 10263 training batches, 621 validation batches\n",
            "    Iter 0: Train 6.2349 | Val 6.2335 | Val PPL 509.55\n",
            "    Iter 500: Train 5.4036 | Val 5.4801 | Val PPL 239.88\n",
            "    Iter 1000: Train 5.1602 | Val 5.0869 | Val PPL 161.89\n",
            "    Iter 1500: Train 4.2778 | Val 4.7706 | Val PPL 117.99\n",
            "    Iter 2000: Train 4.9657 | Val 4.5493 | Val PPL 94.57\n",
            "    Iter 2500: Train 4.6198 | Val 4.4390 | Val PPL 84.69\n",
            "    Iter 3000: Train 4.2545 | Val 4.3695 | Val PPL 79.01\n",
            "    Iter 3500: Train 4.1385 | Val 4.3097 | Val PPL 74.42\n",
            "    Iter 4000: Train 4.9908 | Val 4.2433 | Val PPL 69.64\n",
            "    Iter 4500: Train 4.5388 | Val 4.2135 | Val PPL 67.59\n",
            "    Iter 5000: Train 4.3594 | Val 4.1776 | Val PPL 65.21\n",
            "    Iter 5500: Train 4.5304 | Val 4.1608 | Val PPL 64.12\n",
            "    Iter 6000: Train 4.1386 | Val 4.1317 | Val PPL 62.28\n",
            "    Iter 6500: Train 3.8470 | Val 4.1234 | Val PPL 61.77\n",
            "    Iter 7000: Train 4.3087 | Val 4.1035 | Val PPL 60.55\n",
            "    Iter 7500: Train 3.9564 | Val 4.0920 | Val PPL 59.86\n",
            "    Iter 8000: Train 4.2019 | Val 4.0749 | Val PPL 58.84\n",
            "    Iter 8500: Train 4.7037 | Val 4.0768 | Val PPL 58.96\n",
            "    Iter 9000: Train 4.3602 | Val 4.0665 | Val PPL 58.35\n",
            "    Iter 9500: Train 4.8911 | Val 4.0582 | Val PPL 57.87\n",
            "    Plot saved: task3_plots/task3_merges500_lr0.001.png\n",
            "    val_ppl=58.88 | test_ppl=63.48\n",
            "    [Generated]: to be or not to be withy him to this winday d but should not this whose rest\n",
            "  Training with LR=0.0005\n",
            "    Config: emb_dim=64, batch=32\n",
            "    Prepared 10263 training batches, 621 validation batches\n",
            "    Iter 0: Train 6.2348 | Val 6.2343 | Val PPL 509.93\n",
            "    Iter 500: Train 5.8765 | Val 5.9090 | Val PPL 368.33\n",
            "    Iter 1000: Train 5.2135 | Val 5.4204 | Val PPL 225.96\n",
            "    Iter 1500: Train 4.9187 | Val 5.1616 | Val PPL 174.45\n",
            "    Iter 2000: Train 4.9430 | Val 4.9794 | Val PPL 145.38\n",
            "    Iter 2500: Train 4.7356 | Val 4.8267 | Val PPL 124.80\n",
            "    Iter 3000: Train 4.5939 | Val 4.7192 | Val PPL 112.08\n",
            "    Iter 3500: Train 4.6873 | Val 4.6195 | Val PPL 101.44\n",
            "    Iter 4000: Train 4.9167 | Val 4.5308 | Val PPL 92.83\n",
            "    Iter 4500: Train 4.3407 | Val 4.4588 | Val PPL 86.39\n",
            "    Iter 5000: Train 4.4529 | Val 4.3880 | Val PPL 80.48\n",
            "    Iter 5500: Train 4.2597 | Val 4.3340 | Val PPL 76.25\n",
            "    Iter 6000: Train 4.1733 | Val 4.2858 | Val PPL 72.66\n",
            "    Iter 6500: Train 4.0286 | Val 4.2490 | Val PPL 70.04\n",
            "    Iter 7000: Train 4.4155 | Val 4.2362 | Val PPL 69.15\n",
            "    Iter 7500: Train 4.5398 | Val 4.1957 | Val PPL 66.40\n",
            "    Iter 8000: Train 3.9006 | Val 4.1766 | Val PPL 65.14\n",
            "    Iter 8500: Train 4.0605 | Val 4.1561 | Val PPL 63.82\n",
            "    Iter 9000: Train 3.9499 | Val 4.1333 | Val PPL 62.38\n",
            "    Iter 9500: Train 3.9783 | Val 4.1088 | Val PPL 60.87\n",
            "    Plot saved: task3_plots/task3_merges500_lr0.0005.png\n",
            "    val_ppl=66.06 | test_ppl=72.20\n",
            "    [Generated]: to be or not to be s honeysither d that i do my was bs have we dr\n",
            "\n",
            "BPE merges=1000\n",
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "  Vocab size=635, train tokens=311187\n",
            "  Training with LR=0.001\n",
            "    Config: emb_dim=64, batch=32\n",
            "    Prepared 9724 training batches, 590 validation batches\n",
            "    Iter 0: Train 6.4532 | Val 6.4534 | Val PPL 634.85\n",
            "    Iter 500: Train 5.5199 | Val 5.5841 | Val PPL 266.16\n",
            "    Iter 1000: Train 5.4977 | Val 5.1663 | Val PPL 175.27\n",
            "    Iter 1500: Train 5.3374 | Val 4.8821 | Val PPL 131.91\n",
            "    Iter 2000: Train 4.8662 | Val 4.7273 | Val PPL 112.98\n",
            "    Iter 2500: Train 4.6143 | Val 4.6043 | Val PPL 99.91\n",
            "    Iter 3000: Train 4.4979 | Val 4.5163 | Val PPL 91.49\n",
            "    Iter 3500: Train 4.7819 | Val 4.4518 | Val PPL 85.78\n",
            "    Iter 4000: Train 4.3310 | Val 4.4045 | Val PPL 81.82\n",
            "    Iter 4500: Train 4.5538 | Val 4.3520 | Val PPL 77.64\n",
            "    Iter 5000: Train 4.4416 | Val 4.3216 | Val PPL 75.31\n",
            "    Iter 5500: Train 4.6808 | Val 4.2860 | Val PPL 72.68\n",
            "    Iter 6000: Train 4.7981 | Val 4.2576 | Val PPL 70.64\n",
            "    Iter 6500: Train 4.2606 | Val 4.2446 | Val PPL 69.73\n",
            "    Iter 7000: Train 3.9813 | Val 4.2178 | Val PPL 67.89\n",
            "    Iter 7500: Train 4.5777 | Val 4.2330 | Val PPL 68.92\n",
            "    Iter 8000: Train 4.2054 | Val 4.2179 | Val PPL 67.89\n",
            "    Iter 8500: Train 4.3212 | Val 4.1865 | Val PPL 65.80\n",
            "    Iter 9000: Train 4.0653 | Val 4.1710 | Val PPL 64.78\n",
            "    Iter 9500: Train 4.1835 | Val 4.1799 | Val PPL 65.36\n",
            "    Plot saved: task3_plots/task3_merges1000_lr0.001.png\n",
            "    val_ppl=66.67 | test_ppl=73.64\n",
            "    [Generated]: to be or not to be bear his meet ahoratly ck ble to the ha\n",
            "  Training with LR=0.0005\n",
            "    Config: emb_dim=64, batch=32\n",
            "    Prepared 9724 training batches, 590 validation batches\n",
            "    Iter 0: Train 6.4550 | Val 6.4532 | Val PPL 634.76\n",
            "    Iter 500: Train 6.1157 | Val 6.1017 | Val PPL 446.60\n",
            "    Iter 1000: Train 5.4655 | Val 5.6477 | Val PPL 283.63\n",
            "    Iter 1500: Train 5.2724 | Val 5.4590 | Val PPL 234.87\n",
            "    Iter 2000: Train 5.0444 | Val 5.3020 | Val PPL 200.74\n",
            "    Iter 2500: Train 5.1950 | Val 5.1432 | Val PPL 171.25\n",
            "    Iter 3000: Train 4.7226 | Val 5.0104 | Val PPL 149.96\n",
            "    Iter 3500: Train 4.6715 | Val 4.8990 | Val PPL 134.15\n",
            "    Iter 4000: Train 4.6825 | Val 4.7997 | Val PPL 121.47\n",
            "    Iter 4500: Train 4.4172 | Val 4.7223 | Val PPL 112.43\n",
            "    Iter 5000: Train 4.8834 | Val 4.6566 | Val PPL 105.27\n",
            "    Iter 5500: Train 4.4491 | Val 4.5985 | Val PPL 99.33\n",
            "    Iter 6000: Train 4.5017 | Val 4.5473 | Val PPL 94.37\n",
            "    Iter 6500: Train 4.1241 | Val 4.5045 | Val PPL 90.43\n",
            "    Iter 7000: Train 3.9269 | Val 4.4763 | Val PPL 87.91\n",
            "    Iter 7500: Train 4.6446 | Val 4.4451 | Val PPL 85.21\n",
            "    Iter 8000: Train 4.8198 | Val 4.4203 | Val PPL 83.12\n",
            "    Iter 8500: Train 4.6136 | Val 4.3999 | Val PPL 81.45\n",
            "    Iter 9000: Train 4.3875 | Val 4.3677 | Val PPL 78.86\n",
            "    Iter 9500: Train 4.4067 | Val 4.3515 | Val PPL 77.59\n",
            "    Plot saved: task3_plots/task3_merges1000_lr0.0005.png\n",
            "    val_ppl=76.71 | test_ppl=85.17\n",
            "    [Generated]: to be or not to be this poss she his hamay with you no mlet but we in well i\n",
            "\n",
            "BPE merges=2000\n",
            "Loaded cached BPE: 2000 merges, lower_nopunct normalization\n",
            "  Vocab size=628, train tokens=310187\n",
            "  Training with LR=0.001\n",
            "    Config: emb_dim=64, batch=32\n",
            "    Prepared 9693 training batches, 590 validation batches\n",
            "    Iter 0: Train 6.4437 | Val 6.4426 | Val PPL 628.07\n",
            "    Iter 500: Train 5.6960 | Val 5.6543 | Val PPL 285.53\n",
            "    Iter 1000: Train 5.3083 | Val 5.2731 | Val PPL 195.01\n",
            "    Iter 1500: Train 4.9933 | Val 4.9872 | Val PPL 146.52\n",
            "    Iter 2000: Train 5.0429 | Val 4.8007 | Val PPL 121.59\n",
            "    Iter 2500: Train 5.3209 | Val 4.6588 | Val PPL 105.51\n",
            "    Iter 3000: Train 4.5207 | Val 4.5616 | Val PPL 95.73\n",
            "    Iter 3500: Train 4.5965 | Val 4.4825 | Val PPL 88.45\n",
            "    Iter 4000: Train 4.4417 | Val 4.4138 | Val PPL 82.58\n",
            "    Iter 4500: Train 4.4756 | Val 4.3886 | Val PPL 80.52\n",
            "    Iter 5000: Train 4.0265 | Val 4.3554 | Val PPL 77.90\n",
            "    Iter 5500: Train 4.4453 | Val 4.3270 | Val PPL 75.72\n",
            "    Iter 6000: Train 4.2374 | Val 4.2907 | Val PPL 73.02\n",
            "    Iter 6500: Train 4.6303 | Val 4.2763 | Val PPL 71.98\n",
            "    Iter 7000: Train 4.3967 | Val 4.2776 | Val PPL 72.06\n",
            "    Iter 7500: Train 4.4207 | Val 4.2644 | Val PPL 71.12\n",
            "    Iter 8000: Train 4.1649 | Val 4.2717 | Val PPL 71.64\n",
            "    Iter 8500: Train 4.3633 | Val 4.2525 | Val PPL 70.28\n",
            "    Iter 9000: Train 4.1363 | Val 4.2566 | Val PPL 70.57\n",
            "    Iter 9500: Train 4.0014 | Val 4.2506 | Val PPL 70.15\n",
            "    Plot saved: task3_plots/task3_merges2000_lr0.001.png\n",
            "    val_ppl=67.05 | test_ppl=72.98\n",
            "    [Generated]: to be or not to be in our she lord py laun d same to her hoisous\n",
            "  Training with LR=0.0005\n",
            "    Config: emb_dim=64, batch=32\n",
            "    Prepared 9693 training batches, 590 validation batches\n",
            "    Iter 0: Train 6.4406 | Val 6.4417 | Val PPL 627.48\n",
            "    Iter 500: Train 6.0683 | Val 6.1033 | Val PPL 447.34\n",
            "    Iter 1000: Train 5.6507 | Val 5.5726 | Val PPL 263.11\n",
            "    Iter 1500: Train 5.3872 | Val 5.3486 | Val PPL 210.32\n",
            "    Iter 2000: Train 5.1505 | Val 5.1736 | Val PPL 176.55\n",
            "    Iter 2500: Train 5.3499 | Val 5.0369 | Val PPL 154.00\n",
            "    Iter 3000: Train 4.7837 | Val 4.9251 | Val PPL 137.70\n",
            "    Iter 3500: Train 5.0631 | Val 4.8203 | Val PPL 124.00\n",
            "    Iter 4000: Train 5.1594 | Val 4.7418 | Val PPL 114.64\n",
            "    Iter 4500: Train 4.3652 | Val 4.6818 | Val PPL 107.97\n",
            "    Iter 5000: Train 4.9184 | Val 4.6237 | Val PPL 101.87\n",
            "    Iter 5500: Train 5.0674 | Val 4.5748 | Val PPL 97.01\n",
            "    Iter 6000: Train 4.8301 | Val 4.5306 | Val PPL 92.81\n",
            "    Iter 6500: Train 4.5018 | Val 4.4909 | Val PPL 89.20\n",
            "    Iter 7000: Train 4.2313 | Val 4.4711 | Val PPL 87.45\n",
            "    Iter 7500: Train 4.0059 | Val 4.4380 | Val PPL 84.60\n",
            "    Iter 8000: Train 3.9724 | Val 4.4073 | Val PPL 82.05\n",
            "    Iter 8500: Train 4.3253 | Val 4.3888 | Val PPL 80.54\n",
            "    Iter 9000: Train 4.8567 | Val 4.3873 | Val PPL 80.42\n",
            "    Iter 9500: Train 4.4680 | Val 4.3703 | Val PPL 79.07\n",
            "    Plot saved: task3_plots/task3_merges2000_lr0.0005.png\n",
            "    val_ppl=77.01 | test_ppl=84.74\n",
            "    [Generated]: to be or not to be a malrther abrute ditis he man so poing the tr\n",
            "Results saved to task3_results.pkl\n",
            "\n",
            "============================================================\n",
            "SUMMARY:\n",
            "\n",
            "BPE merges=500 (vocab=510)\n",
            "  LR=0.001: val_ppl=58.88 | test_ppl=63.48\n",
            "  LR=0.0005: val_ppl=66.06 | test_ppl=72.20\n",
            "\n",
            "BPE merges=1000 (vocab=635)\n",
            "  LR=0.001: val_ppl=66.67 | test_ppl=73.64\n",
            "  LR=0.0005: val_ppl=76.71 | test_ppl=85.17\n",
            "\n",
            "BPE merges=2000 (vocab=628)\n",
            "  LR=0.001: val_ppl=67.05 | test_ppl=72.98\n",
            "  LR=0.0005: val_ppl=77.01 | test_ppl=84.74\n",
            "\n",
            "Task 3 completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the directory using shell\n",
        "!zip -r task3_plots.zip task3_plots/\n",
        "\n",
        "# Download using Colab's files module\n",
        "from google.colab import files\n",
        "files.download('task3_plots.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "eY05nvpfEeSH",
        "outputId": "84aad9d1-2c24-417f-bc42-3483b03415a8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: task3_plots/ (stored 0%)\n",
            "updating: task3_plots/task3_merges2000_lr5e-05.png (deflated 7%)\n",
            "updating: task3_plots/task3_merges1000_lr0.0001.png (deflated 6%)\n",
            "updating: task3_plots/task3_merges2000_lr0.0005.png (deflated 7%)\n",
            "updating: task3_plots/task3_merges1000_lr5e-05.png (deflated 7%)\n",
            "updating: task3_plots/task3_merges1000_lr0.0005.png (deflated 7%)\n",
            "updating: task3_plots/task3_merges2000_lr0.0001.png (deflated 6%)\n",
            "updating: task3_plots/task3_merges500_lr0.001.png (deflated 6%)\n",
            "updating: task3_plots/task3_merges500_lr0.0005.png (deflated 6%)\n",
            "updating: task3_plots/task3_merges1000_lr0.001.png (deflated 7%)\n",
            "updating: task3_plots/task3_merges2000_lr0.001.png (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_57210e59-b640-4805-b3f4-4d97812308b6\", \"task3_plots.zip\", 1138889)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python task4.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Cn9JkOEf5D",
        "outputId": "38175767-bf4e-4f45-a32a-6c6fd5ea195a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 4: GPT Implementation (Ultra Compact, Robust Early Stopping)\n",
            "==================================================\n",
            "Using 100.0% of each split | chars: train=864424, valid=51965, test=52008\n",
            "\n",
            "BPE merges=500\n",
            "Loaded cached BPE: 500 merges, lower_nopunct normalization\n",
            "  Vocab size=454, train tokens=328429\n",
            "Created 20525 training sequences\n",
            "Created 641 training batches\n",
            "Created 1241 training sequences\n",
            "Created 39 training batches\n",
            "GPT Model: 259,456 parameters\n",
            "Starting training with 641 training batches\n",
            "Iter    0: Train 6.2947 (PPL  541.7) | Val 6.2809 (EMA 6.2809, PPL  534.3) | LR 3.00e-04\n",
            "Iter   50: Train 5.6349 (PPL  280.0) | Val 5.6573 (EMA 6.2185, PPL  286.4) | LR 3.00e-04\n",
            "Iter  100: Train 5.4231 (PPL  226.6) | Val 5.4527 (EMA 6.1420, PPL  233.4) | LR 3.00e-04\n",
            "Iter  150: Train 5.4180 (PPL  225.4) | Val 5.4161 (EMA 6.0694, PPL  225.0) | LR 3.00e-04\n",
            "Iter  200: Train 5.3824 (PPL  217.5) | Val 5.4025 (EMA 6.0027, PPL  222.0) | LR 3.00e-04\n",
            "Iter  250: Train 5.3772 (PPL  216.4) | Val 5.3746 (EMA 5.9399, PPL  215.8) | LR 3.00e-04\n",
            "Iter  300: Train 5.3345 (PPL  207.4) | Val 5.3164 (EMA 5.8775, PPL  203.6) | LR 3.00e-04\n",
            "Iter  350: Train 5.2124 (PPL  183.5) | Val 5.2393 (EMA 5.8137, PPL  188.5) | LR 3.00e-04\n",
            "Iter  400: Train 5.2556 (PPL  191.6) | Val 5.1355 (EMA 5.7459, PPL  169.9) | LR 3.00e-04\n",
            "Iter  450: Train 5.0557 (PPL  156.9) | Val 5.0234 (EMA 5.6736, PPL  151.9) | LR 3.00e-04\n",
            "Iter  500: Train 5.1135 (PPL  166.3) | Val 4.9193 (EMA 5.5982, PPL  136.9) | LR 3.00e-04\n",
            "Iter  550: Train 4.9056 (PPL  135.0) | Val 4.8268 (EMA 5.5211, PPL  124.8) | LR 3.00e-04\n",
            "Iter  600: Train 4.8198 (PPL  123.9) | Val 4.7441 (EMA 5.4434, PPL  114.9) | LR 3.00e-04\n",
            "Iter  650: Train 4.8701 (PPL  130.3) | Val 4.6783 (EMA 5.3669, PPL  107.6) | LR 3.00e-04\n",
            "Iter  700: Train 4.6614 (PPL  105.8) | Val 4.6144 (EMA 5.2916, PPL  100.9) | LR 3.00e-04\n",
            "Iter  750: Train 4.8129 (PPL  123.1) | Val 4.5681 (EMA 5.2193, PPL   96.4) | LR 3.00e-04\n",
            "Iter  800: Train 4.5693 (PPL   96.5) | Val 4.5287 (EMA 5.1502, PPL   92.6) | LR 3.00e-04\n",
            "Iter  850: Train 4.6319 (PPL  102.7) | Val 4.4868 (EMA 5.0839, PPL   88.8) | LR 3.00e-04\n",
            "Iter  900: Train 4.5619 (PPL   95.8) | Val 4.4538 (EMA 5.0209, PPL   86.0) | LR 3.00e-04\n",
            "Iter  950: Train 4.6217 (PPL  101.7) | Val 4.4257 (EMA 4.9613, PPL   83.6) | LR 3.00e-04\n",
            "Iter 1000: Train 4.5874 (PPL   98.2) | Val 4.3958 (EMA 4.9048, PPL   81.1) | LR 3.00e-04\n",
            "Iter 1050: Train 4.5642 (PPL   96.0) | Val 4.3715 (EMA 4.8515, PPL   79.2) | LR 3.00e-04\n",
            "Iter 1100: Train 4.4920 (PPL   89.3) | Val 4.3524 (EMA 4.8016, PPL   77.7) | LR 3.00e-04\n",
            "Iter 1150: Train 4.4373 (PPL   84.5) | Val 4.3327 (EMA 4.7547, PPL   76.2) | LR 3.00e-04\n",
            "Iter 1200: Train 4.5266 (PPL   92.4) | Val 4.3163 (EMA 4.7108, PPL   74.9) | LR 3.00e-04\n",
            "Iter 1250: Train 4.4846 (PPL   88.6) | Val 4.2922 (EMA 4.6690, PPL   73.1) | LR 3.00e-04\n",
            "Iter 1300: Train 4.3704 (PPL   79.1) | Val 4.2769 (EMA 4.6298, PPL   72.0) | LR 3.00e-04\n",
            "Iter 1350: Train 4.3315 (PPL   76.1) | Val 4.2603 (EMA 4.5928, PPL   70.8) | LR 3.00e-04\n",
            "Iter 1400: Train 4.3807 (PPL   79.9) | Val 4.2478 (EMA 4.5583, PPL   70.0) | LR 3.00e-04\n",
            "Iter 1450: Train 4.4084 (PPL   82.1) | Val 4.2312 (EMA 4.5256, PPL   68.8) | LR 3.00e-04\n",
            "Iter 1500: Train 4.3760 (PPL   79.5) | Val 4.2174 (EMA 4.4948, PPL   67.9) | LR 3.00e-04\n",
            "Iter 1550: Train 4.3804 (PPL   79.9) | Val 4.2082 (EMA 4.4661, PPL   67.2) | LR 3.00e-04\n",
            "Iter 1600: Train 4.3265 (PPL   75.7) | Val 4.1931 (EMA 4.4388, PPL   66.2) | LR 3.00e-04\n",
            "Iter 1650: Train 4.3764 (PPL   79.5) | Val 4.1840 (EMA 4.4133, PPL   65.6) | LR 3.00e-04\n",
            "Iter 1700: Train 4.3703 (PPL   79.1) | Val 4.1810 (EMA 4.3901, PPL   65.4) | LR 3.00e-04\n",
            "Iter 1750: Train 4.2579 (PPL   70.7) | Val 4.1636 (EMA 4.3675, PPL   64.3) | LR 3.00e-04\n",
            "Iter 1800: Train 4.3669 (PPL   78.8) | Val 4.1542 (EMA 4.3461, PPL   63.7) | LR 3.00e-04\n",
            "Iter 1850: Train 4.3225 (PPL   75.4) | Val 4.1483 (EMA 4.3263, PPL   63.3) | LR 3.00e-04\n",
            "Iter 1900: Train 4.2461 (PPL   69.8) | Val 4.1393 (EMA 4.3076, PPL   62.8) | LR 3.00e-04\n",
            "Iter 1950: Train 4.2631 (PPL   71.0) | Val 4.1269 (EMA 4.2896, PPL   62.0) | LR 3.00e-04\n",
            "Iter 2000: Train 4.2544 (PPL   70.4) | Val 4.1184 (EMA 4.2724, PPL   61.5) | LR 3.00e-04\n",
            "Iter 2050: Train 4.2557 (PPL   70.5) | Val 4.1090 (EMA 4.2561, PPL   60.9) | LR 3.00e-04\n",
            "Iter 2100: Train 4.2148 (PPL   67.7) | Val 4.1043 (EMA 4.2409, PPL   60.6) | LR 3.00e-04\n",
            "Iter 2150: Train 4.4041 (PPL   81.8) | Val 4.0951 (EMA 4.2263, PPL   60.0) | LR 3.00e-04\n",
            "Iter 2200: Train 4.1884 (PPL   65.9) | Val 4.0872 (EMA 4.2124, PPL   59.6) | LR 3.00e-04\n",
            "Iter 2250: Train 4.2797 (PPL   72.2) | Val 4.0819 (EMA 4.1994, PPL   59.3) | LR 3.00e-04\n",
            "Iter 2300: Train 4.1458 (PPL   63.2) | Val 4.0638 (EMA 4.1858, PPL   58.2) | LR 3.00e-04\n",
            "Iter 2350: Train 4.1448 (PPL   63.1) | Val 4.0610 (EMA 4.1733, PPL   58.0) | LR 3.00e-04\n",
            "Iter 2400: Train 4.1970 (PPL   66.5) | Val 4.0595 (EMA 4.1619, PPL   57.9) | LR 3.00e-04\n",
            "Iter 2450: Train 4.3567 (PPL   78.0) | Val 4.0539 (EMA 4.1511, PPL   57.6) | LR 3.00e-04\n",
            "Iter 2500: Train 4.2789 (PPL   72.2) | Val 4.0422 (EMA 4.1402, PPL   56.9) | LR 3.00e-04\n",
            "Iter 2550: Train 4.1567 (PPL   63.9) | Val 4.0353 (EMA 4.1297, PPL   56.6) | LR 3.00e-04\n",
            "Iter 2600: Train 4.1012 (PPL   60.4) | Val 4.0305 (EMA 4.1198, PPL   56.3) | LR 3.00e-04\n",
            "Iter 2650: Train 4.2312 (PPL   68.8) | Val 4.0285 (EMA 4.1107, PPL   56.2) | LR 3.00e-04\n",
            "Iter 2700: Train 4.3120 (PPL   74.6) | Val 4.0241 (EMA 4.1020, PPL   55.9) | LR 3.00e-04\n",
            "Iter 2750: Train 4.2280 (PPL   68.6) | Val 4.0144 (EMA 4.0933, PPL   55.4) | LR 3.00e-04\n",
            "Iter 2800: Train 4.1196 (PPL   61.5) | Val 4.0129 (EMA 4.0852, PPL   55.3) | LR 3.00e-04\n",
            "Iter 2850: Train 4.1848 (PPL   65.7) | Val 4.0095 (EMA 4.0777, PPL   55.1) | LR 3.00e-04\n",
            "Iter 2900: Train 4.1015 (PPL   60.4) | Val 3.9986 (EMA 4.0697, PPL   54.5) | LR 3.00e-04\n",
            "Iter 2950: Train 4.0261 (PPL   56.0) | Val 3.9990 (EMA 4.0627, PPL   54.5) | LR 3.00e-04\n",
            "  Plot saved: task4_plots/task4_merges500_lr0.0003.png\n",
            "  Final val perplexity: 56.41\n",
            "  [Generated]: to be or not to be not the emilia pplaw the w and to \n",
            "  Model saved: gpt_model_merge500_compact.pt\n",
            "\n",
            "BPE merges=1000\n",
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "  Vocab size=529, train tokens=311187\n",
            "Created 19448 training sequences\n",
            "Created 608 training batches\n",
            "Created 1180 training sequences\n",
            "Created 37 training batches\n",
            "GPT Model: 269,056 parameters\n",
            "Starting training with 608 training batches\n",
            "Iter    0: Train 6.4713 (PPL  646.3) | Val 6.4142 (EMA 6.4142, PPL  610.5) | LR 3.00e-04\n",
            "Iter   50: Train 5.7292 (PPL  307.7) | Val 5.7293 (EMA 6.3457, PPL  307.8) | LR 3.00e-04\n",
            "Iter  100: Train 5.4852 (PPL  241.1) | Val 5.5091 (EMA 6.2621, PPL  246.9) | LR 3.00e-04\n",
            "Iter  150: Train 5.4625 (PPL  235.7) | Val 5.4699 (EMA 6.1828, PPL  237.4) | LR 3.00e-04\n",
            "Iter  200: Train 5.3959 (PPL  220.5) | Val 5.4536 (EMA 6.1099, PPL  233.6) | LR 3.00e-04\n",
            "Iter  250: Train 5.3767 (PPL  216.3) | Val 5.4357 (EMA 6.0425, PPL  229.4) | LR 3.00e-04\n",
            "Iter  300: Train 5.4236 (PPL  226.7) | Val 5.3986 (EMA 5.9781, PPL  221.1) | LR 3.00e-04\n",
            "Iter  350: Train 5.3208 (PPL  204.6) | Val 5.3406 (EMA 5.9143, PPL  208.6) | LR 3.00e-04\n",
            "Iter  400: Train 5.4026 (PPL  222.0) | Val 5.2701 (EMA 5.8499, PPL  194.4) | LR 3.00e-04\n",
            "Iter  450: Train 5.2979 (PPL  199.9) | Val 5.1899 (EMA 5.7839, PPL  179.4) | LR 3.00e-04\n",
            "Iter  500: Train 5.1836 (PPL  178.3) | Val 5.0982 (EMA 5.7153, PPL  163.7) | LR 3.00e-04\n",
            "Iter  550: Train 5.0563 (PPL  157.0) | Val 5.0021 (EMA 5.6440, PPL  148.7) | LR 3.00e-04\n",
            "Iter  600: Train 5.0778 (PPL  160.4) | Val 4.9193 (EMA 5.5715, PPL  136.9) | LR 3.00e-04\n",
            "Iter  650: Train 4.8752 (PPL  131.0) | Val 4.8426 (EMA 5.4986, PPL  126.8) | LR 3.00e-04\n",
            "Iter  700: Train 4.9783 (PPL  145.2) | Val 4.7848 (EMA 5.4273, PPL  119.7) | LR 3.00e-04\n",
            "Iter  750: Train 4.8068 (PPL  122.3) | Val 4.7275 (EMA 5.3573, PPL  113.0) | LR 3.00e-04\n",
            "Iter  800: Train 4.8992 (PPL  134.2) | Val 4.6835 (EMA 5.2899, PPL  108.1) | LR 3.00e-04\n",
            "Iter  850: Train 4.7191 (PPL  112.1) | Val 4.6524 (EMA 5.2262, PPL  104.8) | LR 3.00e-04\n",
            "Iter  900: Train 4.7775 (PPL  118.8) | Val 4.6141 (EMA 5.1650, PPL  100.9) | LR 3.00e-04\n",
            "Iter  950: Train 4.6970 (PPL  109.6) | Val 4.5803 (EMA 5.1065, PPL   97.5) | LR 3.00e-04\n",
            "Iter 1000: Train 4.6272 (PPL  102.2) | Val 4.5550 (EMA 5.0513, PPL   95.1) | LR 3.00e-04\n",
            "Iter 1050: Train 4.5742 (PPL   96.9) | Val 4.5328 (EMA 4.9995, PPL   93.0) | LR 3.00e-04\n",
            "Iter 1100: Train 4.7165 (PPL  111.8) | Val 4.5093 (EMA 4.9505, PPL   90.9) | LR 3.00e-04\n",
            "Iter 1150: Train 4.5748 (PPL   97.0) | Val 4.4834 (EMA 4.9038, PPL   88.5) | LR 3.00e-04\n",
            "Iter 1200: Train 4.6083 (PPL  100.3) | Val 4.4697 (EMA 4.8604, PPL   87.3) | LR 3.00e-04\n",
            "Iter 1250: Train 4.5532 (PPL   94.9) | Val 4.4433 (EMA 4.8186, PPL   85.1) | LR 3.00e-04\n",
            "Iter 1300: Train 4.5036 (PPL   90.3) | Val 4.4296 (EMA 4.7797, PPL   83.9) | LR 3.00e-04\n",
            "Iter 1350: Train 4.5443 (PPL   94.1) | Val 4.4147 (EMA 4.7432, PPL   82.7) | LR 3.00e-04\n",
            "Iter 1400: Train 4.6301 (PPL  102.5) | Val 4.4001 (EMA 4.7089, PPL   81.5) | LR 3.00e-04\n",
            "Iter 1450: Train 4.4514 (PPL   85.7) | Val 4.3838 (EMA 4.6764, PPL   80.1) | LR 3.00e-04\n",
            "Iter 1500: Train 4.5440 (PPL   94.1) | Val 4.3713 (EMA 4.6459, PPL   79.1) | LR 3.00e-04\n",
            "Iter 1550: Train 4.4230 (PPL   83.3) | Val 4.3588 (EMA 4.6172, PPL   78.2) | LR 3.00e-04\n",
            "Iter 1600: Train 4.4647 (PPL   86.9) | Val 4.3480 (EMA 4.5903, PPL   77.3) | LR 3.00e-04\n",
            "Iter 1650: Train 4.4073 (PPL   82.0) | Val 4.3354 (EMA 4.5648, PPL   76.4) | LR 3.00e-04\n",
            "Iter 1700: Train 4.3571 (PPL   78.0) | Val 4.3239 (EMA 4.5407, PPL   75.5) | LR 3.00e-04\n",
            "Iter 1750: Train 4.4572 (PPL   86.2) | Val 4.3075 (EMA 4.5174, PPL   74.3) | LR 3.00e-04\n",
            "Iter 1800: Train 4.3594 (PPL   78.2) | Val 4.3012 (EMA 4.4958, PPL   73.8) | LR 3.00e-04\n",
            "Iter 1850: Train 4.4107 (PPL   82.3) | Val 4.2935 (EMA 4.4755, PPL   73.2) | LR 3.00e-04\n",
            "Iter 1900: Train 4.3867 (PPL   80.4) | Val 4.2819 (EMA 4.4562, PPL   72.4) | LR 3.00e-04\n",
            "Iter 1950: Train 4.3315 (PPL   76.1) | Val 4.2762 (EMA 4.4382, PPL   72.0) | LR 3.00e-04\n",
            "Iter 2000: Train 4.4780 (PPL   88.1) | Val 4.2636 (EMA 4.4207, PPL   71.1) | LR 3.00e-04\n",
            "Iter 2050: Train 4.3959 (PPL   81.1) | Val 4.2595 (EMA 4.4046, PPL   70.8) | LR 3.00e-04\n",
            "Iter 2100: Train 4.3457 (PPL   77.1) | Val 4.2468 (EMA 4.3888, PPL   69.9) | LR 3.00e-04\n",
            "Iter 2150: Train 4.4049 (PPL   81.9) | Val 4.2377 (EMA 4.3737, PPL   69.2) | LR 3.00e-04\n",
            "Iter 2200: Train 4.3729 (PPL   79.3) | Val 4.2325 (EMA 4.3596, PPL   68.9) | LR 3.00e-04\n",
            "Iter 2250: Train 4.3096 (PPL   74.4) | Val 4.2303 (EMA 4.3467, PPL   68.7) | LR 3.00e-04\n",
            "Iter 2300: Train 4.3933 (PPL   80.9) | Val 4.2178 (EMA 4.3338, PPL   67.9) | LR 3.00e-04\n",
            "Iter 2350: Train 4.3641 (PPL   78.6) | Val 4.2109 (EMA 4.3215, PPL   67.4) | LR 3.00e-04\n",
            "Iter 2400: Train 4.3666 (PPL   78.8) | Val 4.1993 (EMA 4.3093, PPL   66.6) | LR 3.00e-04\n",
            "Iter 2450: Train 4.3712 (PPL   79.1) | Val 4.1963 (EMA 4.2980, PPL   66.4) | LR 3.00e-04\n",
            "Iter 2500: Train 4.3243 (PPL   75.5) | Val 4.1862 (EMA 4.2868, PPL   65.8) | LR 3.00e-04\n",
            "Iter 2550: Train 4.4357 (PPL   84.4) | Val 4.1829 (EMA 4.2764, PPL   65.6) | LR 3.00e-04\n",
            "Iter 2600: Train 4.2272 (PPL   68.5) | Val 4.1738 (EMA 4.2661, PPL   65.0) | LR 3.00e-04\n",
            "Iter 2650: Train 4.3407 (PPL   76.8) | Val 4.1612 (EMA 4.2556, PPL   64.1) | LR 3.00e-04\n",
            "Iter 2700: Train 4.3985 (PPL   81.3) | Val 4.1600 (EMA 4.2461, PPL   64.1) | LR 3.00e-04\n",
            "Iter 2750: Train 4.0689 (PPL   58.5) | Val 4.1542 (EMA 4.2369, PPL   63.7) | LR 3.00e-04\n",
            "Iter 2800: Train 4.1571 (PPL   63.9) | Val 4.1493 (EMA 4.2281, PPL   63.4) | LR 3.00e-04\n",
            "Iter 2850: Train 4.2697 (PPL   71.5) | Val 4.1484 (EMA 4.2202, PPL   63.3) | LR 3.00e-04\n",
            "Iter 2900: Train 4.4175 (PPL   82.9) | Val 4.1462 (EMA 4.2128, PPL   63.2) | LR 3.00e-04\n",
            "Iter 2950: Train 4.2044 (PPL   67.0) | Val 4.1404 (EMA 4.2055, PPL   62.8) | LR 3.00e-04\n",
            "  Plot saved: task4_plots/task4_merges1000_lr0.0003.png\n",
            "  Final val perplexity: 62.12\n",
            "  [Generated]: to be or not to be son his dye as but that i say pothin\n",
            "  Model saved: gpt_model_merge1000_compact.pt\n",
            "\n",
            "BPE merges=2000\n",
            "Loaded cached BPE: 2000 merges, lower_nopunct normalization\n",
            "  Vocab size=373, train tokens=310187\n",
            "Created 19385 training sequences\n",
            "Created 606 training batches\n",
            "Created 1180 training sequences\n",
            "Created 37 training batches\n",
            "GPT Model: 249,088 parameters\n",
            "Starting training with 606 training batches\n",
            "Iter    0: Train 6.1292 (PPL  459.1) | Val 6.1054 (EMA 6.1054, PPL  448.3) | LR 3.00e-04\n",
            "Iter   50: Train 5.1086 (PPL  165.4) | Val 5.0199 (EMA 5.9968, PPL  151.4) | LR 3.00e-04\n",
            "Iter  100: Train 4.8192 (PPL  123.9) | Val 4.8225 (EMA 5.8794, PPL  124.3) | LR 3.00e-04\n",
            "Iter  150: Train 4.8200 (PPL  124.0) | Val 4.7835 (EMA 5.7698, PPL  119.5) | LR 3.00e-04\n",
            "Iter  200: Train 4.7978 (PPL  121.2) | Val 4.7679 (EMA 5.6696, PPL  117.7) | LR 3.00e-04\n",
            "Iter  250: Train 4.8371 (PPL  126.1) | Val 4.7585 (EMA 5.5785, PPL  116.6) | LR 3.00e-04\n",
            "Iter  300: Train 4.8131 (PPL  123.1) | Val 4.7379 (EMA 5.4944, PPL  114.2) | LR 3.00e-04\n",
            "Iter  350: Train 4.7172 (PPL  111.9) | Val 4.6995 (EMA 5.4149, PPL  109.9) | LR 3.00e-04\n",
            "Iter  400: Train 4.7122 (PPL  111.3) | Val 4.6506 (EMA 5.3385, PPL  104.6) | LR 3.00e-04\n",
            "Iter  450: Train 4.6389 (PPL  103.4) | Val 4.5957 (EMA 5.2642, PPL   99.1) | LR 3.00e-04\n",
            "Iter  500: Train 4.6137 (PPL  100.9) | Val 4.5276 (EMA 5.1906, PPL   92.5) | LR 3.00e-04\n",
            "Iter  550: Train 4.5696 (PPL   96.5) | Val 4.4690 (EMA 5.1184, PPL   87.3) | LR 3.00e-04\n",
            "Iter  600: Train 4.4614 (PPL   86.6) | Val 4.4176 (EMA 5.0483, PPL   82.9) | LR 3.00e-04\n",
            "Iter  650: Train 4.4953 (PPL   89.6) | Val 4.3652 (EMA 4.9800, PPL   78.7) | LR 3.00e-04\n",
            "Iter  700: Train 4.4449 (PPL   85.2) | Val 4.3164 (EMA 4.9137, PPL   74.9) | LR 3.00e-04\n",
            "Iter  750: Train 4.3239 (PPL   75.5) | Val 4.2807 (EMA 4.8504, PPL   72.3) | LR 3.00e-04\n",
            "Iter  800: Train 4.3311 (PPL   76.0) | Val 4.2412 (EMA 4.7895, PPL   69.5) | LR 3.00e-04\n",
            "Iter  850: Train 4.3073 (PPL   74.2) | Val 4.2124 (EMA 4.7317, PPL   67.5) | LR 3.00e-04\n",
            "Iter  900: Train 4.3127 (PPL   74.6) | Val 4.1838 (EMA 4.6770, PPL   65.6) | LR 3.00e-04\n",
            "Iter  950: Train 4.2175 (PPL   67.9) | Val 4.1665 (EMA 4.6259, PPL   64.5) | LR 3.00e-04\n",
            "Iter 1000: Train 4.1866 (PPL   65.8) | Val 4.1468 (EMA 4.5780, PPL   63.2) | LR 3.00e-04\n",
            "Iter 1050: Train 4.2732 (PPL   71.8) | Val 4.1190 (EMA 4.5321, PPL   61.5) | LR 3.00e-04\n",
            "Iter 1100: Train 4.1467 (PPL   63.2) | Val 4.1014 (EMA 4.4890, PPL   60.4) | LR 3.00e-04\n",
            "Iter 1150: Train 4.1464 (PPL   63.2) | Val 4.0904 (EMA 4.4492, PPL   59.8) | LR 3.00e-04\n",
            "Iter 1200: Train 4.1927 (PPL   66.2) | Val 4.0825 (EMA 4.4125, PPL   59.3) | LR 3.00e-04\n",
            "Iter 1250: Train 4.1554 (PPL   63.8) | Val 4.0688 (EMA 4.3781, PPL   58.5) | LR 3.00e-04\n",
            "Iter 1300: Train 4.2198 (PPL   68.0) | Val 4.0573 (EMA 4.3460, PPL   57.8) | LR 3.00e-04\n",
            "Iter 1350: Train 4.0311 (PPL   56.3) | Val 4.0407 (EMA 4.3155, PPL   56.9) | LR 3.00e-04\n",
            "Iter 1400: Train 3.9583 (PPL   52.4) | Val 4.0310 (EMA 4.2871, PPL   56.3) | LR 3.00e-04\n",
            "Iter 1450: Train 4.0866 (PPL   59.5) | Val 4.0240 (EMA 4.2607, PPL   55.9) | LR 3.00e-04\n",
            "Iter 1500: Train 4.1102 (PPL   61.0) | Val 4.0114 (EMA 4.2358, PPL   55.2) | LR 3.00e-04\n",
            "Iter 1550: Train 3.9323 (PPL   51.0) | Val 3.9985 (EMA 4.2121, PPL   54.5) | LR 3.00e-04\n",
            "Iter 1600: Train 4.0606 (PPL   58.0) | Val 3.9961 (EMA 4.1905, PPL   54.4) | LR 3.00e-04\n",
            "Iter 1650: Train 4.0929 (PPL   59.9) | Val 3.9891 (EMA 4.1703, PPL   54.0) | LR 3.00e-04\n",
            "Iter 1700: Train 4.1407 (PPL   62.8) | Val 3.9816 (EMA 4.1515, PPL   53.6) | LR 3.00e-04\n",
            "Iter 1750: Train 4.0184 (PPL   55.6) | Val 3.9687 (EMA 4.1332, PPL   52.9) | LR 3.00e-04\n",
            "Iter 1800: Train 3.9614 (PPL   52.5) | Val 3.9648 (EMA 4.1164, PPL   52.7) | LR 3.00e-04\n",
            "Iter 1850: Train 4.0586 (PPL   57.9) | Val 3.9527 (EMA 4.1000, PPL   52.1) | LR 3.00e-04\n",
            "Iter 1900: Train 3.9652 (PPL   52.7) | Val 3.9537 (EMA 4.0854, PPL   52.1) | LR 3.00e-04\n",
            "Iter 1950: Train 3.9898 (PPL   54.0) | Val 3.9453 (EMA 4.0714, PPL   51.7) | LR 3.00e-04\n",
            "Iter 2000: Train 4.0255 (PPL   56.0) | Val 3.9409 (EMA 4.0583, PPL   51.5) | LR 3.00e-04\n",
            "Iter 2050: Train 4.0413 (PPL   56.9) | Val 3.9329 (EMA 4.0458, PPL   51.1) | LR 3.00e-04\n",
            "Iter 2100: Train 4.0529 (PPL   57.6) | Val 3.9279 (EMA 4.0340, PPL   50.8) | LR 3.00e-04\n",
            "Iter 2150: Train 3.9462 (PPL   51.7) | Val 3.9200 (EMA 4.0226, PPL   50.4) | LR 3.00e-04\n",
            "Iter 2200: Train 4.0081 (PPL   55.0) | Val 3.9133 (EMA 4.0117, PPL   50.1) | LR 3.00e-04\n",
            "Iter 2250: Train 4.1004 (PPL   60.4) | Val 3.9110 (EMA 4.0016, PPL   49.9) | LR 3.00e-04\n",
            "Iter 2300: Train 3.8958 (PPL   49.2) | Val 3.9032 (EMA 3.9918, PPL   49.6) | LR 3.00e-04\n",
            "Iter 2350: Train 4.0318 (PPL   56.4) | Val 3.8982 (EMA 3.9824, PPL   49.3) | LR 3.00e-04\n",
            "Iter 2400: Train 3.9240 (PPL   50.6) | Val 3.8920 (EMA 3.9734, PPL   49.0) | LR 3.00e-04\n",
            "Iter 2450: Train 4.0268 (PPL   56.1) | Val 3.8886 (EMA 3.9649, PPL   48.8) | LR 3.00e-04\n",
            "Iter 2500: Train 3.9182 (PPL   50.3) | Val 3.8859 (EMA 3.9570, PPL   48.7) | LR 3.00e-04\n",
            "Iter 2550: Train 3.9805 (PPL   53.5) | Val 3.8822 (EMA 3.9495, PPL   48.5) | LR 3.00e-04\n",
            "Iter 2600: Train 3.8513 (PPL   47.1) | Val 3.8827 (EMA 3.9428, PPL   48.6) | LR 3.00e-04\n",
            "Iter 2650: Train 3.9493 (PPL   51.9) | Val 3.8734 (EMA 3.9359, PPL   48.1) | LR 3.00e-04\n",
            "Iter 2700: Train 3.8729 (PPL   48.1) | Val 3.8687 (EMA 3.9292, PPL   47.9) | LR 3.00e-04\n",
            "Iter 2750: Train 3.9751 (PPL   53.3) | Val 3.8632 (EMA 3.9226, PPL   47.6) | LR 3.00e-04\n",
            "Iter 2800: Train 3.9879 (PPL   53.9) | Val 3.8585 (EMA 3.9162, PPL   47.4) | LR 3.00e-04\n",
            "Iter 2850: Train 3.9768 (PPL   53.3) | Val 3.8501 (EMA 3.9096, PPL   47.0) | LR 3.00e-04\n",
            "Iter 2900: Train 3.8491 (PPL   47.0) | Val 3.8489 (EMA 3.9035, PPL   46.9) | LR 3.00e-04\n",
            "Iter 2950: Train 3.8669 (PPL   47.8) | Val 3.8459 (EMA 3.8977, PPL   46.8) | LR 3.00e-04\n",
            "  Plot saved: task4_plots/task4_merges2000_lr0.0003.png\n",
            "  Final val perplexity: 46.74\n",
            "  [Generated]: to be or not to be   etch     im w  b\n",
            "  Model saved: gpt_model_merge2000_compact.pt\n",
            "\n",
            "BPE merges=2500\n",
            "Loaded cached BPE: 2500 merges, lower_nopunct normalization\n",
            "  Vocab size=141, train tokens=309687\n",
            "Created 19354 training sequences\n",
            "Created 605 training batches\n",
            "Created 1180 training sequences\n",
            "Created 37 training batches\n",
            "GPT Model: 219,392 parameters\n",
            "Starting training with 605 training batches\n",
            "Iter    0: Train 5.0106 (PPL  150.0) | Val 4.8376 (EMA 4.8376, PPL  126.2) | LR 3.00e-04\n",
            "Iter   50: Train 3.1170 (PPL   22.6) | Val 2.8956 (EMA 4.6434, PPL   18.1) | LR 3.00e-04\n",
            "Iter  100: Train 2.8209 (PPL   16.8) | Val 2.8184 (EMA 4.4609, PPL   16.7) | LR 3.00e-04\n",
            "Iter  150: Train 2.9994 (PPL   20.1) | Val 2.8054 (EMA 4.2953, PPL   16.5) | LR 3.00e-04\n",
            "Iter  200: Train 2.9088 (PPL   18.3) | Val 2.7985 (EMA 4.1457, PPL   16.4) | LR 3.00e-04\n",
            "Iter  250: Train 2.7438 (PPL   15.5) | Val 2.7939 (EMA 4.0105, PPL   16.3) | LR 3.00e-04\n",
            "Iter  300: Train 2.7939 (PPL   16.3) | Val 2.7865 (EMA 3.8881, PPL   16.2) | LR 3.00e-04\n",
            "Iter  350: Train 2.7192 (PPL   15.2) | Val 2.7711 (EMA 3.7764, PPL   16.0) | LR 3.00e-04\n",
            "Iter  400: Train 2.7832 (PPL   16.2) | Val 2.7541 (EMA 3.6742, PPL   15.7) | LR 3.00e-04\n",
            "Iter  450: Train 2.7843 (PPL   16.2) | Val 2.7364 (EMA 3.5804, PPL   15.4) | LR 3.00e-04\n",
            "Iter  500: Train 2.7785 (PPL   16.1) | Val 2.7181 (EMA 3.4942, PPL   15.2) | LR 3.00e-04\n",
            "Iter  550: Train 2.6960 (PPL   14.8) | Val 2.7020 (EMA 3.4149, PPL   14.9) | LR 3.00e-04\n",
            "Iter  600: Train 2.7490 (PPL   15.6) | Val 2.6822 (EMA 3.3417, PPL   14.6) | LR 3.00e-04\n",
            "Iter  650: Train 2.7391 (PPL   15.5) | Val 2.6677 (EMA 3.2743, PPL   14.4) | LR 3.00e-04\n",
            "Iter  700: Train 2.7022 (PPL   14.9) | Val 2.6495 (EMA 3.2118, PPL   14.1) | LR 3.00e-04\n",
            "Iter  750: Train 2.6110 (PPL   13.6) | Val 2.6381 (EMA 3.1544, PPL   14.0) | LR 3.00e-04\n",
            "Iter  800: Train 2.7252 (PPL   15.3) | Val 2.6256 (EMA 3.1015, PPL   13.8) | LR 3.00e-04\n",
            "Iter  850: Train 2.7418 (PPL   15.5) | Val 2.6179 (EMA 3.0532, PPL   13.7) | LR 3.00e-04\n",
            "Iter  900: Train 2.5279 (PPL   12.5) | Val 2.6073 (EMA 3.0086, PPL   13.6) | LR 3.00e-04\n",
            "Iter  950: Train 2.6458 (PPL   14.1) | Val 2.6027 (EMA 2.9680, PPL   13.5) | LR 3.00e-04\n",
            "Iter 1000: Train 2.7115 (PPL   15.1) | Val 2.5956 (EMA 2.9308, PPL   13.4) | LR 3.00e-04\n",
            "Iter 1050: Train 2.6264 (PPL   13.8) | Val 2.5850 (EMA 2.8962, PPL   13.3) | LR 3.00e-04\n",
            "Iter 1100: Train 2.7667 (PPL   15.9) | Val 2.5813 (EMA 2.8647, PPL   13.2) | LR 3.00e-04\n",
            "Iter 1150: Train 2.7805 (PPL   16.1) | Val 2.5761 (EMA 2.8358, PPL   13.1) | LR 3.00e-04\n",
            "Iter 1200: Train 2.6799 (PPL   14.6) | Val 2.5753 (EMA 2.8098, PPL   13.1) | LR 3.00e-04\n",
            "Iter 1250: Train 2.5740 (PPL   13.1) | Val 2.5664 (EMA 2.7854, PPL   13.0) | LR 3.00e-04\n",
            "Iter 1300: Train 2.8775 (PPL   17.8) | Val 2.5650 (EMA 2.7634, PPL   13.0) | LR 3.00e-04\n",
            "Iter 1350: Train 2.6280 (PPL   13.8) | Val 2.5620 (EMA 2.7433, PPL   13.0) | LR 3.00e-04\n",
            "Iter 1400: Train 2.6972 (PPL   14.8) | Val 2.5583 (EMA 2.7248, PPL   12.9) | LR 3.00e-04\n",
            "Iter 1450: Train 2.6416 (PPL   14.0) | Val 2.5589 (EMA 2.7082, PPL   12.9) | LR 3.00e-04\n",
            "Iter 1500: Train 2.5450 (PPL   12.7) | Val 2.5533 (EMA 2.6927, PPL   12.9) | LR 3.00e-04\n",
            "Iter 1550: Train 2.7246 (PPL   15.3) | Val 2.5496 (EMA 2.6784, PPL   12.8) | LR 3.00e-04\n",
            "Iter 1600: Train 2.7468 (PPL   15.6) | Val 2.5496 (EMA 2.6655, PPL   12.8) | LR 3.00e-04\n",
            "Iter 1650: Train 2.5865 (PPL   13.3) | Val 2.5436 (EMA 2.6533, PPL   12.7) | LR 3.00e-04\n",
            "Iter 1700: Train 2.5996 (PPL   13.5) | Val 2.5441 (EMA 2.6424, PPL   12.7) | LR 3.00e-04\n",
            "Iter 1750: Train 2.7482 (PPL   15.6) | Val 2.5403 (EMA 2.6322, PPL   12.7) | LR 3.00e-04\n",
            "Iter 1800: Train 2.5794 (PPL   13.2) | Val 2.5385 (EMA 2.6228, PPL   12.7) | LR 3.00e-04\n",
            "Iter 1850: Train 2.5532 (PPL   12.8) | Val 2.5393 (EMA 2.6145, PPL   12.7) | LR 3.00e-04\n",
            "Iter 1900: Train 2.5243 (PPL   12.5) | Val 2.5316 (EMA 2.6062, PPL   12.6) | LR 3.00e-04\n",
            "Iter 1950: Train 2.6471 (PPL   14.1) | Val 2.5346 (EMA 2.5990, PPL   12.6) | LR 3.00e-04\n",
            "Iter 2000: Train 2.6940 (PPL   14.8) | Val 2.5313 (EMA 2.5922, PPL   12.6) | LR 3.00e-04\n",
            "Iter 2050: Train 2.6125 (PPL   13.6) | Val 2.5256 (EMA 2.5856, PPL   12.5) | LR 3.00e-04\n",
            "Iter 2100: Train 2.5615 (PPL   13.0) | Val 2.5269 (EMA 2.5797, PPL   12.5) | LR 3.00e-04\n",
            "Iter 2150: Train 2.6170 (PPL   13.7) | Val 2.5254 (EMA 2.5743, PPL   12.5) | LR 3.00e-04\n",
            "Iter 2200: Train 2.5184 (PPL   12.4) | Val 2.5249 (EMA 2.5693, PPL   12.5) | LR 3.00e-04\n",
            "Iter 2250: Train 2.5111 (PPL   12.3) | Val 2.5183 (EMA 2.5642, PPL   12.4) | LR 3.00e-04\n",
            "Iter 2300: Train 2.5718 (PPL   13.1) | Val 2.5175 (EMA 2.5596, PPL   12.4) | LR 3.00e-04\n",
            "Iter 2350: Train 2.6712 (PPL   14.5) | Val 2.5163 (EMA 2.5552, PPL   12.4) | LR 3.00e-04\n",
            "Iter 2400: Train 2.6635 (PPL   14.3) | Val 2.5166 (EMA 2.5514, PPL   12.4) | LR 3.00e-04\n",
            "Iter 2450: Train 2.6911 (PPL   14.7) | Val 2.5139 (EMA 2.5476, PPL   12.4) | LR 3.00e-04\n",
            "Iter 2500: Train 2.6284 (PPL   13.9) | Val 2.5124 (EMA 2.5441, PPL   12.3) | LR 3.00e-04\n",
            "Iter 2550: Train 2.6458 (PPL   14.1) | Val 2.5084 (EMA 2.5405, PPL   12.3) | LR 3.00e-04\n",
            "Iter 2600: Train 2.6453 (PPL   14.1) | Val 2.5043 (EMA 2.5369, PPL   12.2) | LR 3.00e-04\n",
            "Iter 2650: Train 2.6130 (PPL   13.6) | Val 2.5042 (EMA 2.5336, PPL   12.2) | LR 3.00e-04\n",
            "Iter 2700: Train 2.5740 (PPL   13.1) | Val 2.5068 (EMA 2.5310, PPL   12.3) | LR 3.00e-04\n",
            "Iter 2750: Train 2.5927 (PPL   13.4) | Val 2.5063 (EMA 2.5285, PPL   12.3) | LR 3.00e-04\n",
            "Iter 2800: Train 2.5678 (PPL   13.0) | Val 2.5035 (EMA 2.5260, PPL   12.2) | LR 3.00e-04\n",
            "Iter 2850: Train 2.5544 (PPL   12.9) | Val 2.5026 (EMA 2.5237, PPL   12.2) | LR 3.00e-04\n",
            "Iter 2900: Train 2.4936 (PPL   12.1) | Val 2.4960 (EMA 2.5209, PPL   12.1) | LR 3.00e-04\n",
            "Iter 2950: Train 2.5424 (PPL   12.7) | Val 2.4967 (EMA 2.5185, PPL   12.1) | LR 3.00e-04\n",
            "  Plot saved: task4_plots/task4_merges2500_lr0.0003.png\n",
            "  Final val perplexity: 12.20\n",
            "  [Generated]: to aor ato aaaaaaaaaaaaaaao\n",
            "  Model saved: gpt_model_merge2500_compact.pt\n",
            "\n",
            "Task 4 completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the directory using shell\n",
        "!zip -r task4_plots.zip task4_plots/\n",
        "\n",
        "# Download using Colab's files module\n",
        "from google.colab import files\n",
        "files.download('task4_plots.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "T29I6r3E47Er",
        "outputId": "dc44ca35-9169-4392-b423-e23aafcac9a3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: task4_plots/ (stored 0%)\n",
            "updating: task4_plots/task4_merges2000_lr0.0003.png (deflated 8%)\n",
            "updating: task4_plots/task4_merges1000_lr0.0003.png (deflated 7%)\n",
            "updating: task4_plots/task4_merges500_lr0.0003.png (deflated 8%)\n",
            "updating: task4_plots/task4_merges2500_lr0.0003.png (deflated 9%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6e981b0a-a3c3-4224-b619-a4389bdd4083\", \"task4_plots.zip\", 405113)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Tests"
      ],
      "metadata": {
        "id": "j9OG8TEYEjpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ngram Models"
      ],
      "metadata": {
        "id": "DWWwWWvTEmQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from utils import load_cached_bpe\n",
        "\n",
        "# Import the NGramModel class first\n",
        "from task2 import NGramModel\n",
        "\n",
        "def generate_best_models(context):\n",
        "    \"\"\"Generate from best model for each merge count\"\"\"\n",
        "\n",
        "    # Best models: 1000 merges -> 3-gram, 2000 merges -> 2-gram\n",
        "    for merges, (file, model_type) in [(500, ('task2_fixed_500_2.pkl', '3-gram')),\n",
        "                                       (2000, ('task2_fixed_1000_2.pkl', '2-gram'))]:\n",
        "        bpe = load_cached_bpe(merges, \"lower_nopunct\")\n",
        "        with open(file, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        generated = model.generate(bpe, context, max_tokens=20, temperature=0.7)\n",
        "        print(f\"{merges} merges ({model_type}): {generated}\")\n",
        "        print(\"\\n\")\n",
        "# Usage:\n",
        "generate_best_models(context=\"i think therefore\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlk8Tyu4ElbG",
        "outputId": "74dbfe3d-4360-40b4-af46-f1941a31bb5d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached BPE: 500 merges, lower_nopunct normalization\n",
            "500 merges (3-gram): i think therefore wintly where ury hermona mlet me i have was t \n",
            "\n",
            "\n",
            "Loaded cached BPE: 2000 merges, lower_nopunct normalization\n",
            "2000 merges (2-gram): i think therefore my dear egypt him good made them stuncan fairet it ps a vius exeunt \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Bigram"
      ],
      "metadata": {
        "id": "xyW_YO1iEp9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from task3 import NeuralBigramModel, generate_text\n",
        "from utils import load_cached_bpe, load_and_slice_data, normalize_generation_context, GEN_CONTEXT\n",
        "import torch\n",
        "\n",
        "def generate_task3_best(context=\"in patience lies the fortress of hope\"):\n",
        "    # Best configs you noted\n",
        "    configs = [\n",
        "        (1000, 0.0005, 36.89, 36.55),\n",
        "        (2000, 0.0005, 37.56, 37.96),\n",
        "    ]\n",
        "\n",
        "    context = normalize_generation_context(context)\n",
        "\n",
        "    for merges, lr, val_ppl, test_ppl in configs:\n",
        "        try:\n",
        "            bpe = load_cached_bpe(merges, \"lower_nopunct\")\n",
        "            if bpe is None:\n",
        "                print(f\"Error with {merges} merges: cached BPE not found. Run Task 1 for caching.\")\n",
        "                continue\n",
        "\n",
        "            # Rebuild the same vocab/id maps as in task3 training for consistency\n",
        "            train_text, valid_text, test_text = load_and_slice_data(1.0)\n",
        "            train_tokens = bpe.encode(train_text)\n",
        "            valid_tokens = bpe.encode(valid_text)\n",
        "            test_tokens  = bpe.encode(test_text)\n",
        "            ctx_tokens   = bpe.encode(normalize_generation_context(GEN_CONTEXT), preserve_edge_spaces=True)\n",
        "\n",
        "            all_tokens = sorted(set(train_tokens) | set(valid_tokens) | set(test_tokens) | set(ctx_tokens))\n",
        "            bpe.vocab = all_tokens\n",
        "            bpe.token2id = {tok: i for i, tok in enumerate(bpe.vocab)}\n",
        "            bpe.id2token = {i: tok for tok, i in bpe.token2id.items()}\n",
        "            vocab_size = len(bpe.vocab)\n",
        "\n",
        "            # Model must match training config\n",
        "            model = NeuralBigramModel(vocab_size=vocab_size, embedding_dim=64)\n",
        "            ckpt_path = f\"task3_{merges}_lr{lr}_final.pt\"\n",
        "            state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "            model.load_state_dict(state, strict=True)\n",
        "            model.eval()\n",
        "\n",
        "            generated = generate_text(\n",
        "                model=model,\n",
        "                bpe=bpe,\n",
        "                context=context,\n",
        "                max_tokens=20,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            print(f\"{merges} merges (LR={lr}, Val PPL={val_ppl:.2f}, Test PPL={test_ppl:.2f}): {generated}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {merges} merges: {e}\")\n",
        "\n",
        "# Usage:\n",
        "generate_task3_best(context=\"I think therefore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-UE71scErKK",
        "outputId": "2c73d0fb-24f8-47d2-91b6-639438ac0edb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "Using 100.0% of each split | chars: train=864424, valid=51965, test=52008\n",
            "1000 merges (LR=0.0005, Val PPL=36.89, Test PPL=36.55): i think therefore have ple other that i know dus and mont wicish i to do\n",
            "\n",
            "Loaded cached BPE: 2000 merges, lower_nopunct normalization\n",
            "Using 100.0% of each split | chars: train=864424, valid=51965, test=52008\n",
            "2000 merges (LR=0.0005, Val PPL=37.56, Test PPL=37.96): i think therefore is the scian atch d foor sdement to youraw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT"
      ],
      "metadata": {
        "id": "fCMoYbFbEs_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gpt_models(context=\"in patience lies the fortress of hope\"):\n",
        "    import torch\n",
        "    from utils import load_cached_bpe, normalize_generation_context\n",
        "    from task4 import GPTModel\n",
        "\n",
        "    models = [\n",
        "        (500, \"gpt_model_merge500_compact.pt\"),\n",
        "        (1000, \"gpt_model_merge1000_compact.pt\"),\n",
        "    ]\n",
        "\n",
        "    for merges, model_file in models:\n",
        "        print(f\"\\nGPT {merges} merges:\")\n",
        "        bpe = load_cached_bpe(merges, \"lower_nopunct\")\n",
        "        if bpe is None:\n",
        "            print(\"  Missing cached BPE. Run Task 1 first.\")\n",
        "            continue\n",
        "\n",
        "        checkpoint = torch.load(model_file, map_location=\"cpu\")\n",
        "        model_config = checkpoint[\"model_config\"]\n",
        "\n",
        "        model = GPTModel(\n",
        "            vocab_size=checkpoint[\"vocab_size\"],\n",
        "            n_embd=model_config[\"n_embd\"],\n",
        "            n_head=model_config[\"n_head\"],\n",
        "            n_layer=model_config[\"n_layer\"],\n",
        "            chunk_size=model_config[\"chunk_size\"],\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        model.eval()\n",
        "\n",
        "        # Prefer mappings stored with the checkpoint (exact training vocab)\n",
        "        token2id = checkpoint.get(\"token_to_id\", bpe.token2id)\n",
        "        id2token = checkpoint.get(\"id_to_token\", bpe.id2token)\n",
        "\n",
        "        # Normalize context same way as training generation\n",
        "        ctx = normalize_generation_context(context)\n",
        "        context_tokens = bpe.encode(ctx)  # default norm is 'lower_nopunct'\n",
        "        unk_id = 0  # fallback index, consistent with task4 training code\n",
        "        context_ids = [token2id.get(tok, unk_id) for tok in context_tokens]\n",
        "\n",
        "        try:\n",
        "            generated_ids = model.generate(context_ids, max_tokens=20, temperature=0.6, top_k=50)\n",
        "            generated_tokens = [id2token.get(tid, \"<UNK>\") for tid in generated_ids]\n",
        "            generated_text = bpe.decode(generated_tokens)\n",
        "            print(f\"  {generated_text}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Generation error: {e}\")\n",
        "\n",
        "# Usage:\n",
        "generate_gpt_models(\"What light through yonder window breaks, when\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3xHT30nEtqU",
        "outputId": "865db9d3-737e-4000-b6c6-a2631e752d32"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT 500 merges:\n",
            "Loaded cached BPE: 500 merges, lower_nopunct normalization\n",
            "GPT Model: 259,456 parameters\n",
            "  what light through yonder window breaks when the deriet the der in his hona i foget \n",
            "\n",
            "GPT 1000 merges:\n",
            "Loaded cached BPE: 1000 merges, lower_nopunct normalization\n",
            "GPT Model: 269,056 parameters\n",
            "  what light through yonder window breaks when it the  croil and not not sill of them and in my h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwkn5wGNsoP0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}